{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 rows from E:\\AYUSH\\amex-default-prediction\\train_data.csv\n",
      "Loaded 10000 rows from E:\\AYUSH\\amex-default-prediction\\test_data.csv\n",
      "Loaded 10000 rows from E:\\AYUSH\\amex-default-prediction\\train_labels.csv\n",
      "Loaded 10000 rows from E:\\AYUSH\\amex-default-prediction\\sample_submission.csv\n",
      "\n",
      "Top 10 rows of Train Data with Serial Numbers:\n",
      "                                         customer_ID         S_2       P_2  \\\n",
      "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-03-09  0.938469   \n",
      "1  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-04-07  0.936665   \n",
      "2  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-05-28  0.954180   \n",
      "3  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-06-13  0.960384   \n",
      "4  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-07-16  0.947248   \n",
      "5  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-08-04  0.945964   \n",
      "6  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-09-18  0.940705   \n",
      "7  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-10-08  0.914767   \n",
      "8  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-11-20  0.950845   \n",
      "9  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-12-04  0.868580   \n",
      "\n",
      "       D_39       B_1       B_2       R_1       S_3      D_41       B_3  ...  \\\n",
      "0  0.001733  0.008724  1.006838  0.009228  0.124035  0.008771  0.004709  ...   \n",
      "1  0.005775  0.004923  1.000653  0.006151  0.126750  0.000798  0.002714  ...   \n",
      "2  0.091505  0.021655  1.009672  0.006815  0.123977  0.007598  0.009423  ...   \n",
      "3  0.002455  0.013683  1.002700  0.001373  0.117169  0.000685  0.005531  ...   \n",
      "4  0.002483  0.015193  1.000727  0.007605  0.117325  0.004653  0.009312  ...   \n",
      "5  0.001746  0.007863  1.005006  0.004220  0.110946  0.009857  0.009866  ...   \n",
      "6  0.002183  0.018859  1.008024  0.004509  0.103329  0.006603  0.000783  ...   \n",
      "7  0.003029  0.014324  1.000242  0.000263  0.108115  0.009527  0.007836  ...   \n",
      "8  0.009896  0.016888  1.003995  0.001789  0.102792  0.002519  0.009817  ...   \n",
      "9  0.001082  0.001930  1.007504  0.001772  0.100470  0.004626  0.006073  ...   \n",
      "\n",
      "   D_137  D_138     D_139     D_140     D_141  D_142     D_143     D_144  \\\n",
      "0    NaN    NaN  0.002427  0.003706  0.003818    NaN  0.000569  0.000610   \n",
      "1    NaN    NaN  0.003954  0.003167  0.005032    NaN  0.009576  0.005492   \n",
      "2    NaN    NaN  0.003269  0.007329  0.000427    NaN  0.003429  0.006986   \n",
      "3    NaN    NaN  0.006117  0.004516  0.003200    NaN  0.008419  0.006527   \n",
      "4    NaN    NaN  0.003671  0.004946  0.008889    NaN  0.001670  0.008126   \n",
      "5    NaN    NaN  0.001924  0.008598  0.004529    NaN  0.000674  0.002223   \n",
      "6    NaN    NaN  0.001336  0.004361  0.009387    NaN  0.007727  0.007661   \n",
      "7    NaN    NaN  0.002397  0.008452  0.005553    NaN  0.001831  0.009616   \n",
      "8    NaN    NaN  0.009742  0.003968  0.007945    NaN  0.008722  0.004369   \n",
      "9    NaN    NaN  0.003611  0.009607  0.007266    NaN  0.008763  0.004753   \n",
      "\n",
      "      D_145  Serial  \n",
      "0  0.002674       1  \n",
      "1  0.009217       2  \n",
      "2  0.002603       3  \n",
      "3  0.009600       4  \n",
      "4  0.009827       5  \n",
      "5  0.002884       6  \n",
      "6  0.002225       7  \n",
      "7  0.007385       8  \n",
      "8  0.000995       9  \n",
      "9  0.009068      10  \n",
      "\n",
      "[10 rows x 191 columns]\n",
      "\n",
      "Top 10 rows of Test Data with Serial Numbers:\n",
      "                                         customer_ID         S_2       P_2  \\\n",
      "0  00000469ba478561f23a92a868bd366de6f6527a684c9a...  2019-02-19  0.631315   \n",
      "1  00000469ba478561f23a92a868bd366de6f6527a684c9a...  2019-03-25  0.587042   \n",
      "2  00000469ba478561f23a92a868bd366de6f6527a684c9a...  2019-04-25  0.609056   \n",
      "3  00000469ba478561f23a92a868bd366de6f6527a684c9a...  2019-05-20  0.614911   \n",
      "4  00000469ba478561f23a92a868bd366de6f6527a684c9a...  2019-06-15  0.591673   \n",
      "5  00000469ba478561f23a92a868bd366de6f6527a684c9a...  2019-07-13  0.587472   \n",
      "6  00000469ba478561f23a92a868bd366de6f6527a684c9a...  2019-08-16  0.625006   \n",
      "7  00000469ba478561f23a92a868bd366de6f6527a684c9a...  2019-09-29  0.597074   \n",
      "8  00000469ba478561f23a92a868bd366de6f6527a684c9a...  2019-10-12  0.568930   \n",
      "9  00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...  2018-04-22  0.894195   \n",
      "\n",
      "       D_39       B_1       B_2       R_1       S_3      D_41       B_3  ...  \\\n",
      "0  0.001912  0.010728  0.814497  0.007547  0.168651  0.009971  0.002347  ...   \n",
      "1  0.005275  0.011026  0.810848  0.001817  0.241389  0.000166  0.009132  ...   \n",
      "2  0.003326  0.016390  1.004620  0.000114  0.266976  0.004196  0.004192  ...   \n",
      "3  0.009065  0.021672  0.816549  0.009722  0.188947  0.004123  0.015325  ...   \n",
      "4  0.238794  0.015923  0.810456  0.002026  0.180035  0.000731  0.011281  ...   \n",
      "5  0.005827  0.007959  1.001009  0.008814  0.173701  0.000653  0.009779  ...   \n",
      "6  0.238704  0.013420  0.816605  0.009849  0.170644  0.002495  0.019999  ...   \n",
      "7  0.003537  0.017426  1.000670  0.005143  0.158006  0.000985  0.011962  ...   \n",
      "8  0.121385  0.010779  1.009347  0.006923  0.149413  0.000396  0.003576  ...   \n",
      "9  0.325657  0.020970  1.001803  0.005125  0.073243  0.005347  0.001597  ...   \n",
      "\n",
      "   D_137  D_138     D_139     D_140     D_141  D_142     D_143     D_144  \\\n",
      "0    NaN    NaN       NaN  0.004669       NaN    NaN       NaN  0.008281   \n",
      "1    NaN    NaN  0.000142  0.004940  0.009021    NaN  0.003695  0.003753   \n",
      "2    NaN    NaN  0.000074  0.002114  0.004656    NaN  0.003155  0.002156   \n",
      "3    NaN    NaN  0.004743  0.006392  0.002890    NaN  0.006044  0.005206   \n",
      "4    NaN    NaN  0.008133  0.004329  0.008384    NaN  0.001008  0.007421   \n",
      "5    NaN    NaN  0.007001  0.003962  0.005530    NaN  0.009870  0.009667   \n",
      "6    NaN    NaN  0.001132  0.007676  0.005410    NaN  0.006762  0.005664   \n",
      "7    NaN    NaN  0.008730  0.000628  0.002821    NaN  0.004141  0.005733   \n",
      "8    NaN    NaN  0.005912  0.001250  0.006543    NaN  0.009160  0.003690   \n",
      "9    NaN    NaN  0.008065  0.009861  0.009535    NaN  0.003964  0.008436   \n",
      "\n",
      "      D_145  Serial  \n",
      "0       NaN       1  \n",
      "1  0.001460       2  \n",
      "2  0.006482       3  \n",
      "3  0.007855       4  \n",
      "4  0.009471       5  \n",
      "5  0.005398       6  \n",
      "6  0.002627       7  \n",
      "7  0.005657       8  \n",
      "8  0.003219       9  \n",
      "9  0.008323      10  \n",
      "\n",
      "[10 rows x 191 columns]\n",
      "\n",
      "Top 10 rows of Train Data with Serial Numbers:\n",
      "                                         customer_ID  target  Serial\n",
      "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...       0       1\n",
      "1  00000fd6641609c6ece5454664794f0340ad84dddce9a2...       0       2\n",
      "2  00001b22f846c82c51f6e3958ccd81970162bae8b007e8...       0       3\n",
      "3  000041bdba6ecadd89a52d11886e8eaaec9325906c9723...       0       4\n",
      "4  00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...       0       5\n",
      "5  000084e5023181993c2e1b665ac88dbb1ce9ef621ec537...       0       6\n",
      "6  000098081fde4fd64bc4d503a5d6f86a0aedc425c96f52...       0       7\n",
      "7  0000d17a1447b25a01e42e1ac56b091bb7cbb06317be4c...       0       8\n",
      "8  0000f99513770170a1aba690daeeb8a96da4a39f11fc27...       1       9\n",
      "9  00013181a0c5fc8f1ea38cd2b90fe8ad2fa8cad9d9f13e...       1      10\n",
      "\n",
      "Top 10 rows of Train Data with Serial Numbers:\n",
      "                                         customer_ID  prediction  Serial\n",
      "0  00000469ba478561f23a92a868bd366de6f6527a684c9a...           0       1\n",
      "1  00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...           0       2\n",
      "2  0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...           0       3\n",
      "3  00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...           0       4\n",
      "4  00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...           0       5\n",
      "5  00004ffe6e01e1b688170bbd108da8351bc4c316eacfef...           0       6\n",
      "6  00007cfcce97abfa0b4fa0647986157281d01d3ab90de9...           0       7\n",
      "7  000089cc2a30dad8e6ba39126f9d86df6088c9f975093a...           0       8\n",
      "8  00008f50a1dd76fa211ba36a2b0d5a1b201e4134a5fd53...           0       9\n",
      "9  0000b48a4f27dc1d61e78d081678e811620300b88eb3ab...           0      10\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the dataset\n",
    "def load_data_in_chunks(file_path, nrows=10000):\n",
    "    \"\"\"Load a specific number of rows from a large CSV file and add a Serial Number column.\"\"\"\n",
    "    chunk = pd.read_csv(file_path, nrows=nrows)\n",
    "    chunk['Serial'] = range(1, len(chunk) + 1)  # Adding Serial Number column starting from 1\n",
    "    print(f\"Loaded {nrows} rows from {file_path}\")\n",
    "    return chunk\n",
    "\n",
    "train_data = load_data_in_chunks(r\"E:\\AYUSH\\amex-default-prediction\\train_data.csv\", nrows=10000)\n",
    "test = load_data_in_chunks(r\"E:\\AYUSH\\amex-default-prediction\\test_data.csv\", nrows=10000)\n",
    "train_labels = load_data_in_chunks(r\"E:\\AYUSH\\amex-default-prediction\\train_labels.csv\", nrows=10000)\n",
    "sample_submission = load_data_in_chunks(r\"E:\\AYUSH\\amex-default-prediction\\sample_submission.csv\", nrows=10000)\n",
    "\n",
    "\n",
    "print(\"\\nTop 10 rows of Train Data with Serial Numbers:\")\n",
    "print(train_data.head(10))\n",
    "print(\"\\nTop 10 rows of Test Data with Serial Numbers:\")\n",
    "print(test.head(10))\n",
    "print(\"\\nTop 10 rows of Train Data with Serial Numbers:\")\n",
    "print(train_labels.head(10))\n",
    "print(\"\\nTop 10 rows of Train Data with Serial Numbers:\")\n",
    "print(sample_submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Information of the Dataset:\n",
      "--------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Columns: 191 entries, customer_ID to Serial\n",
      "dtypes: float64(185), int64(2), object(4)\n",
      "memory usage: 14.6+ MB\n",
      "None\n",
      "\n",
      "Basic Statistics of Numerical Columns:\n",
      "--------------------------------------------------\n",
      "          count         mean          std           min          25%  \\\n",
      "P_2      9936.0     0.650498     0.252416 -2.569212e-01     0.471264   \n",
      "D_39    10000.0     0.157031     0.275827  8.701630e-07     0.004574   \n",
      "B_1     10000.0     0.126147     0.212428 -1.414690e-01     0.009126   \n",
      "B_2     10000.0     0.617122     0.403145  3.432000e-05     0.091656   \n",
      "R_1     10000.0     0.074035     0.219977  3.031180e-06     0.002863   \n",
      "...         ...          ...          ...           ...          ...   \n",
      "D_142    1572.0     0.370263     0.249771 -8.804185e-03     0.150429   \n",
      "D_143    9847.0     0.162935     0.364742  2.172557e-06     0.002939   \n",
      "D_144    9937.0     0.046364     0.174197  7.773110e-07     0.002674   \n",
      "D_145    9847.0     0.057846     0.226945  1.577788e-07     0.002966   \n",
      "Serial  10000.0  5000.500000  2886.895680  1.000000e+00  2500.750000   \n",
      "\n",
      "                50%          75%           max  \n",
      "P_2        0.690195     0.866060      1.009926  \n",
      "D_39       0.009296     0.238015      4.268383  \n",
      "B_1        0.032948     0.122675      1.323411  \n",
      "B_2        0.814048     1.002262      1.009999  \n",
      "R_1        0.005726     0.008528      2.259283  \n",
      "...             ...          ...           ...  \n",
      "D_142      0.352169     0.559880      1.185992  \n",
      "D_143      0.005944     0.008907      1.010000  \n",
      "D_144      0.005323     0.008105      1.342362  \n",
      "D_145      0.005932     0.008878      4.282032  \n",
      "Serial  5000.500000  7500.250000  10000.000000  \n",
      "\n",
      "[187 rows x 8 columns]\n",
      "\n",
      "Missing Values in Top Columns:\n",
      "--------------------------------------------------\n",
      "D_87     9990\n",
      "D_88     9980\n",
      "D_111    9975\n",
      "D_110    9975\n",
      "B_39     9972\n",
      "D_108    9944\n",
      "D_73     9926\n",
      "B_42     9831\n",
      "D_138    9628\n",
      "D_136    9628\n",
      "dtype: int64\n",
      "\n",
      "'Target' column is not present in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Exploratory Data Analysis (EDA)\n",
    "def explore_data(data):\n",
    "    \"\"\"Perform basic exploration to understand the data.\"\"\"\n",
    "    print(\"Basic Information of the Dataset:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(data.info())\n",
    "    print(\"\\nBasic Statistics of Numerical Columns:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(data.describe().T)\n",
    "    \n",
    "    print(\"\\nMissing Values in Top Columns:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(data.isnull().sum().sort_values(ascending=False).head(10))\n",
    "    \n",
    "    # Check if 'target' column exists before analyzing its distribution\n",
    "    if 'target' in data.columns:\n",
    "        print(\"\\nTarget Distribution:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(data['target'].value_counts(normalize=True))\n",
    "        \n",
    "        # Visualizing target distribution\n",
    "        sns.countplot(x='target', data=data)\n",
    "        plt.title(\"Target Distribution\")\n",
    "        plt.xlabel(\"Target Class\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n'Target' column is not present in the dataset.\")\n",
    "\n",
    "# Call the function\n",
    "explore_data(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Handling Missing Values\n",
    "def handle_missing_values(train, test):\n",
    "    \"\"\"Fill missing values with a placeholder.\"\"\"\n",
    "    train.fillna(-999, inplace=True)\n",
    "    test.fillna(-999, inplace=True)\n",
    "    return train, test\n",
    "\n",
    "train, test = handle_missing_values(train_data, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Encoding Categorical Variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_categorical(train, test):\n",
    "    \"\"\"Label encode categorical features.\"\"\"\n",
    "    categorical_cols = train.select_dtypes(include=['object']).columns\n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    # Apply encoding for each categorical column\n",
    "    for col in categorical_cols:\n",
    "        # Fit on training data and transform both train and test sets\n",
    "        train[col] = encoder.fit_transform(train[col].astype(str))\n",
    "        \n",
    "        # Transform test data using the same encoder\n",
    "        # We use 'fit_transform' only on training data to avoid test data leakage\n",
    "        test[col] = encoder.transform(test[col].astype(str))\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Apply the encoding function to train and test datasets\n",
    "train, test = encode_categorical(train_data, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaling completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_features(train, test):\n",
    "    \"\"\"Standardize numerical features with aligned columns.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Drop non-numeric columns first\n",
    "    train_numeric = train.select_dtypes(include=['int64', 'float64'])\n",
    "    test_numeric = test.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    # Align columns between train and test\n",
    "    train_numeric, test_numeric = train_numeric.align(test_numeric, join='inner', axis=1)\n",
    "\n",
    "    # Fit scaler on train data and transform both train and test\n",
    "    train_scaled = scaler.fit_transform(train_numeric)\n",
    "    test_scaled = scaler.transform(test_numeric)\n",
    "\n",
    "    # Replace the numerical columns in the original train and test DataFrames\n",
    "    train[train_numeric.columns] = train_scaled\n",
    "    test[train_numeric.columns] = test_scaled\n",
    "\n",
    "    return train, test\n",
    "\n",
    "# Apply feature scaling\n",
    "train, test = scale_features(train_data, test)\n",
    "\n",
    "print(\"Feature scaling completed successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Splitting Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_data(train):\n",
    "    \"\"\"Split data into training and validation sets.\"\"\"\n",
    "    if not isinstance(train, pd.DataFrame):\n",
    "        raise ValueError(\"Input should be a DataFrame.\")\n",
    "    \n",
    "    X = train.drop(columns=['target', 'id'], errors='ignore')\n",
    "    y = train['target'] if 'target' in train.columns else None\n",
    "    \n",
    "    if y is None:\n",
    "        raise ValueError(\"Column 'target' not found in the DataFrame.\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Custom Evaluation Metric\n",
    "def amex_metric(y_true, y_pred):\n",
    "    \"\"\"Define the AMEX evaluation metric.\"\"\"\n",
    "    def top_four_percent_captured(y_true, y_pred):\n",
    "        df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\n",
    "        df = df.sort_values('y_pred', ascending=False)\n",
    "        df['weight'] = df['y_true'].apply(lambda x: 20 if x == 1 else 1)\n",
    "        cutoff = int(0.04 * df['weight'].sum())\n",
    "        return df.iloc[:cutoff]['y_true'].sum() / df['y_true'].sum()\n",
    "\n",
    "    gini = 2 * roc_auc_score(y_true, y_pred) - 1\n",
    "    return 0.5 * (gini + top_four_percent_captured(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Train Baseline Logistic Regression\n",
    "def split_train_data(train):\n",
    "    \"\"\"Split data into training and validation sets.\"\"\"\n",
    "    # Check for available columns\n",
    "    print(\"Columns in train:\", train.columns)\n",
    "\n",
    "    # Drop 'target' or 'id' only if they exist\n",
    "    drop_cols = [col for col in ['target', 'id'] if col in train.columns]\n",
    "    X = train.drop(columns=drop_cols)  # Drop valid columns only\n",
    "    \n",
    "    y = train['target'] if 'target' in train.columns else None  # Ensure 'target' exists\n",
    "\n",
    "    if y is None:\n",
    "        raise ValueError(\"Column 'target' not found in the dataset!\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Train Advanced XGBoost Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_train_data(train):\n",
    "    target_column = 'target'  # Update this to the correct column name if needed\n",
    "    if target_column not in train.columns:\n",
    "        raise ValueError(f\"Column '{target_column}' not found in the dataset!\")\n",
    "\n",
    "    y = train[target_column]\n",
    "    X = train.drop(columns=[target_column])\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The input data is empty. Please check the data.\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Analyze Feature Importance\n",
    "# Ensure 'customer_ID' has the same data type in both datasets\n",
    "train_data[\"customer_ID\"] = train_data[\"customer_ID\"].astype(str)\n",
    "train_labels[\"customer_ID\"] = train_labels[\"customer_ID\"].astype(str)\n",
    "\n",
    "# Check if 'customer_ID' is unique before setting it as the index\n",
    "if train_data[\"customer_ID\"].is_unique and train_labels[\"customer_ID\"].is_unique:\n",
    "    # Set 'customer_ID' as the index for both DataFrames before concatenation\n",
    "    train = pd.concat([train_data.set_index(\"customer_ID\"), \n",
    "                       train_labels.set_index(\"customer_ID\")], axis=1).reset_index()\n",
    "else:\n",
    "    # If 'customer_ID' is not unique, merge based on 'customer_ID' instead\n",
    "    train = pd.merge(train_data, train_labels, on=\"customer_ID\", how=\"inner\")\n",
    "\n",
    "# Prepare data for training\n",
    "X = train.drop(columns=[\"customer_ID\", \"target\"], errors=\"ignore\")  # Features\n",
    "y = train[\"target\"]  # Target column\n",
    "\n",
    "# Ensure that X and y are not empty\n",
    "if len(X) > 0 and len(y) > 0:\n",
    "    # Train-validation split with a valid test_size and train_size\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Step 5: Train XGBoost Model\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\", \n",
    "        eval_metric=\"auc\", \n",
    "        learning_rate=0.05, \n",
    "        max_depth=6, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8, \n",
    "        random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=True)\n",
    "\n",
    "    # Step 6: Feature Importance Visualization\n",
    "    def analyze_features(model, X_val):\n",
    "        \"\"\"\n",
    "        Visualize feature importance using SHAP and XGBoost.\n",
    "        \"\"\"\n",
    "        # XGBoost Feature Importance\n",
    "        xgb.plot_importance(model)\n",
    "        plt.title(\"XGBoost Feature Importance\")\n",
    "        plt.show()\n",
    "\n",
    "        # SHAP Feature Importance\n",
    "        explainer = shap.Explainer(model, X_val)\n",
    "        shap_values = explainer(X_val)\n",
    "        shap.summary_plot(shap_values, X_val)\n",
    "\n",
    "    # Step 7: Call the function\n",
    "    analyze_features(xgb_model, X_val)\n",
    "\n",
    "else:\n",
    "    print(\"Error: The input data is empty. Please check the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'id' column not found in the test DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Generate Submission File\n",
    "def encode_categorical(train, test):\n",
    "    \"\"\"Label encode categorical features.\"\"\"\n",
    "    categorical_cols = train.select_dtypes(include=['object']).columns\n",
    "    encoder = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        train[col] = encoder.fit_transform(train[col].astype(str))\n",
    "        test[col] = encoder.transform(test[col].astype(str))\n",
    "    return train, test\n",
    "\n",
    "# Step 2: Train XGBoost Model without early stopping\n",
    "def train_xgb_model(train_data, train_labels):\n",
    "    \"\"\"Train XGBoost model on the training data.\"\"\"\n",
    "    # Prepare training data\n",
    "    X = train_data.drop(columns=['target'], errors='ignore')  # Drop 'target' if it exists\n",
    "    y = train_labels['target']\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize the XGBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\", \n",
    "        learning_rate=0.05, \n",
    "        max_depth=6, \n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model without early stopping\n",
    "    xgb_model.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "    return xgb_model\n",
    "\n",
    "# Step 3: Generate Submission File\n",
    "def generate_submission(test, xgb_model):\n",
    "    \"\"\"Create the final submission file.\"\"\"\n",
    "    # Check if the 'id' column exists in the test DataFrame\n",
    "    if 'id' not in test.columns:\n",
    "        print(\"Error: 'id' column not found in the test DataFrame.\")\n",
    "        return\n",
    "    \n",
    "    # Prepare test data (drop 'id' column and other non-predictive columns)\n",
    "    X_test = test.drop(columns=['id'], errors='ignore')\n",
    "    \n",
    "    # Convert test data to DMatrix format for XGBoost\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    \n",
    "    # Make predictions (ensure binary classification, or adjust as necessary)\n",
    "    predictions = xgb_model.predict(dtest)\n",
    "    \n",
    "    # If the model is binary classification, convert probabilities to 0 or 1\n",
    "    if predictions.shape[0] > 1:  # In case of output being probabilities\n",
    "        predictions = (predictions > 0.5).astype(int)  # Threshold at 0.5\n",
    "    \n",
    "    # Create the submission DataFrame\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test['id'],\n",
    "        'prediction': predictions\n",
    "    })\n",
    "    \n",
    "    # Save the submission file to the specified path\n",
    "    submission.to_csv(r'E:\\AYUSH\\amex-default-prediction\\sample_submission.csv', index=False)\n",
    "    print(\"Submission file saved as sample_submission.csv\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'train_data', 'train_labels', and 'test' are already loaded with the respective data\n",
    "\n",
    "# Step 4: Encode categorical features (if needed)\n",
    "train_data, test = encode_categorical(train_data, test)\n",
    "\n",
    "# Step 5: Train the XGBoost model\n",
    "xgb_model = train_xgb_model(train_data, train_labels)  # Ensure 'train_data' and 'train_labels' are defined\n",
    "\n",
    "# Step 6: Generate the submission file\n",
    "generate_submission(test, xgb_model)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
